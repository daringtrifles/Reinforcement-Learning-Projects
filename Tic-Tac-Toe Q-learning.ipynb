{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02af332",
   "metadata": {},
   "source": [
    "A lot of the code is shared between this notebook and the previous one, so I won't explain it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44b4fd9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:42.935745Z",
     "iopub.status.busy": "2022-08-22T16:19:42.935289Z",
     "iopub.status.idle": "2022-08-22T16:19:43.277617Z",
     "shell.execute_reply": "2022-08-22T16:19:43.276453Z"
    },
    "papermill": {
     "duration": 0.354138,
     "end_time": "2022-08-22T16:19:43.282782",
     "exception": false,
     "start_time": "2022-08-22T16:19:42.928644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'reaction']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "# Create the game environment\n",
    "env = make(\"tictactoe\", debug=True)\n",
    "\n",
    "# List of available default agents\n",
    "print(list(env.agents))\n",
    "\n",
    "q={} #q-table\n",
    "seen = {} #a map storing all states that have been seen before\n",
    "for i in range (3**10):\n",
    "    q[i] = [0] * 9\n",
    "# Sets q-table values to 0.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e758a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.292521Z",
     "iopub.status.busy": "2022-08-22T16:19:43.291498Z",
     "iopub.status.idle": "2022-08-22T16:19:43.296963Z",
     "shell.execute_reply": "2022-08-22T16:19:43.296272Z"
    },
    "papermill": {
     "duration": 0.012181,
     "end_time": "2022-08-22T16:19:43.298899",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.286718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hash (array):\n",
    "    x = 0\n",
    "    for i in range (9):\n",
    "        x += 3**i * array[i]\n",
    "    return x\n",
    "# Converts tictactoe board to number (which the q-table takes as an input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1867017d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.307969Z",
     "iopub.status.busy": "2022-08-22T16:19:43.307370Z",
     "iopub.status.idle": "2022-08-22T16:19:43.315712Z",
     "shell.execute_reply": "2022-08-22T16:19:43.314991Z"
    },
    "papermill": {
     "duration": 0.015069,
     "end_time": "2022-08-22T16:19:43.317691",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.302622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result (board): #this method takes in the final board state of a game that has finished and returns whether the result is a win for board 1 or board 2 or if the result is a draw.\n",
    "    # 100 means board 1 won\n",
    "    # 0 means draw\n",
    "    # -100 means board 2 won\n",
    "    for i in range (3):\n",
    "        if board[i*3:i*3+3].count(1) == 3: return 100\n",
    "        if board[i*3:i*3+3].count(2) == 3: return -100\n",
    "    for i in range(3):\n",
    "        if board[i] == board[i+3] == board[i+6] == 1: return 100\n",
    "        if board[i] == board[i+3] == board[i+6] == 2: return -100 \n",
    "    if board[0] == board[4] == board[8] == 1: return 100\n",
    "    if board[2] == board[4] == board[6] == 1: return 100\n",
    "    if board[0] == board[4] == board[8] == 2: return -100\n",
    "    if board[2] == board[4] == board[6] == 2: return -100\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e647071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.326826Z",
     "iopub.status.busy": "2022-08-22T16:19:43.326109Z",
     "iopub.status.idle": "2022-08-22T16:19:43.333486Z",
     "shell.execute_reply": "2022-08-22T16:19:43.332625Z"
    },
    "papermill": {
     "duration": 0.014189,
     "end_time": "2022-08-22T16:19:43.335699",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.321510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkDone(board): #Checks if the game is done\n",
    "    if board.count(0) == 0: return True\n",
    "    for i in range (3):\n",
    "        if board[i*3:i*3+3].count(1) == 3 or board[i*3:i*3+3].count(2) == 3: return True\n",
    "    for i in range(3):\n",
    "        if board[i] == board[i+3] == board[i+6] != 0:\n",
    "            return True\n",
    "    if board[0] == board[4] == board[8] != 0: return True\n",
    "    if board[2] == board[4] == board[6] != 0: return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df8068",
   "metadata": {},
   "source": [
    "To get an in depth explanation of q-learning, click on this link: https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning#:~:text=Q%2Dlearning%20is%20a%20model,next%20action%20to%20be%20taken. \n",
    "\n",
    "For the purposes of this notebook, I'll explain q-learning in simpler terms and how I apply it to tic-tac-toe. To implement q-learning, a q-table is needed. Think of a q-table as a 2d list with each row representing a different position and each column representing a different move. Each value in the q-table signifies how good or bad a particular position is. A more positive value indicates a better position, and 0 indicates a draw. The values in this table are filled recursively, with the final state positions being given either a very positive score (5000) for a win, 0 for a draw, and a very negative score for a loss (-5000). For every move that is illegal, a very, very, negative score is assigned (-1000000) and the game terminates. Every other value in the table is updated by a linear combination of the current value and the maximum value after the corresponding move.\n",
    "\n",
    "\n",
    "The agent starts off with a q-table filed with zeroes, and slowly updates it as it plays against a random opponent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "695b09a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.344846Z",
     "iopub.status.busy": "2022-08-22T16:19:43.344136Z",
     "iopub.status.idle": "2022-08-22T16:19:43.355468Z",
     "shell.execute_reply": "2022-08-22T16:19:43.354300Z"
    },
    "papermill": {
     "duration": 0.018107,
     "end_time": "2022-08-22T16:19:43.357637",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.339530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_learning_agent(obs, config):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.2\n",
    "    epsilon = max(0.2,0.5 - 0.5/games * t)\n",
    "    mark = obs.mark\n",
    "    valid_moves = [i for i in range (len(obs.board)) if obs.board[i] == 0]\n",
    "    scores = []\n",
    "    move = -1\n",
    "    reward = 0\n",
    "    copyBoard = [1]*9\n",
    "    if np.random.uniform(0, 1)< epsilon:\n",
    "        move= np.random.randint(9)\n",
    "    else:\n",
    "        bestScore = max(q[hash(obs.board)])\n",
    "        move = q[hash(obs.board)].index(bestScore)\n",
    "    if move not in valid_moves:\n",
    "        reward -= 1000000\n",
    "    else:\n",
    "        copyBoard = obs.board.copy()\n",
    "        copyBoard[move] = mark\n",
    "        if (checkDone(copyBoard)):\n",
    "            if result(copyBoard) == 100:\n",
    "                reward += 5000\n",
    "        else: \n",
    "            newmark = max(1, (mark + 1)%3)\n",
    "            copyBoard[random_feeder(copyBoard)] = newmark \n",
    "            if (checkDone(copyBoard)):\n",
    "                if (result(copyBoard)) == 0:\n",
    "                    reward -= 50\n",
    "                else: reward -= 5000\n",
    "    #update q-table\n",
    "    q[hash(obs.board)][move] = (1-alpha)*q[hash(obs.board)][move] + alpha * (reward + gamma*max(q[hash(copyBoard)]))  \n",
    "    return move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e95c1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.366866Z",
     "iopub.status.busy": "2022-08-22T16:19:43.366488Z",
     "iopub.status.idle": "2022-08-22T16:19:43.372428Z",
     "shell.execute_reply": "2022-08-22T16:19:43.371281Z"
    },
    "papermill": {
     "duration": 0.012945,
     "end_time": "2022-08-22T16:19:43.374405",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.361460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global rand_move\n",
    "def random_feeder (board): #feeds a random move to  random_agent.\n",
    "    global rand_move\n",
    "    valid_moves = [i for i in range (len(board)) if board[i] == 0]\n",
    "    rand_move = int(np.random.choice(valid_moves))\n",
    "    return rand_move\n",
    "\n",
    "def random_agent (obs, config): #Used to train the agent\n",
    "    #abc = random_feeder(obs.board) #COMMENT THIS OUT\n",
    "    return rand_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94036cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:19:43.383961Z",
     "iopub.status.busy": "2022-08-22T16:19:43.383264Z",
     "iopub.status.idle": "2022-08-22T19:05:16.477119Z",
     "shell.execute_reply": "2022-08-22T19:05:16.476042Z"
    },
    "papermill": {
     "duration": 9933.101598,
     "end_time": "2022-08-22T19:05:16.479928",
     "exception": false,
     "start_time": "2022-08-22T16:19:43.378330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t=0\n",
    "games = 100000\n",
    "for i in range (games):\n",
    "    t+=1\n",
    "    env.run([q_learning_agent, random_agent])\n",
    "    env.run([random_agent, q_learning_agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01dd48dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T19:05:20.771419Z",
     "iopub.status.busy": "2022-08-22T19:05:20.770979Z",
     "iopub.status.idle": "2022-08-22T19:05:20.783078Z",
     "shell.execute_reply": "2022-08-22T19:05:20.782281Z"
    },
    "papermill": {
     "duration": 2.052826,
     "end_time": "2022-08-22T19:05:20.785488",
     "exception": false,
     "start_time": "2022-08-22T19:05:18.732662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_learning_player(obs, config):\n",
    "    seen[hash(obs.board)] = 1\n",
    "    bestScore = max(q[hash(obs.board)])\n",
    "    move = q[hash(obs.board)].index(bestScore)\n",
    "    return move\n",
    "\n",
    "import numpy as np\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 3, 'columns': 3, 'inarow': 3}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"tictactoe\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"tictactoe\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n",
    "def run(agent1, agent2, n_rounds=100):\n",
    "    config = {'rows': 3, 'columns': 3, 'inarow': 3}\n",
    "    outcomes = evaluate(\"tictactoe\", [agent1, agent2], config, [], n_rounds//2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f2e092b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T19:05:25.063123Z",
     "iopub.status.busy": "2022-08-22T19:05:25.062412Z",
     "iopub.status.idle": "2022-08-22T19:06:13.858155Z",
     "shell.execute_reply": "2022-08-22T19:06:13.857051Z"
    },
    "papermill": {
     "duration": 53.119772,
     "end_time": "2022-08-22T19:06:16.032625",
     "exception": false,
     "start_time": "2022-08-22T19:05:22.912853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.0\n",
      "Agent 2 Win Percentage: 0.95\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make, evaluate\n",
    "env = make(\"tictactoe\")\n",
    "get_win_percentages(agent1='random', agent2=q_learning_player, n_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2ab9d",
   "metadata": {
    "papermill": {
     "duration": 2.02802,
     "end_time": "2022-08-22T19:06:20.310984",
     "exception": false,
     "start_time": "2022-08-22T19:06:18.282964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, the win percentage is 0.05 higher than the previous minimax agent, which is a significant 50% reduction in draws."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10018.616951,
   "end_time": "2022-08-22T19:06:32.241717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-22T16:19:33.624766",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
